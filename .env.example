# ==============================================================================
# NTPU LineBot Go - Environment Configuration
# ==============================================================================

# ------------------------------------------------------------------------------
# LINE Bot Configuration (Required)
# ------------------------------------------------------------------------------
LINE_CHANNEL_ACCESS_TOKEN=your_access_token_here
LINE_CHANNEL_SECRET=your_channel_secret_here

# ------------------------------------------------------------------------------
# LLM Configuration (Optional)
# At least one API key required to enable NLU intent parsing and query expansion
# ------------------------------------------------------------------------------
# Get Cerebras API key from: https://cloud.cerebras.ai/
#CEREBRAS_API_KEY=your_cerebras_api_key_here

# Get Gemini API key from: https://aistudio.google.com/apikey
#GEMINI_API_KEY=your_gemini_api_key_here

# Get Groq API key from: https://console.groq.com/keys
#GROQ_API_KEY=your_groq_api_key_here

# Provider fallback order (comma-separated, default: gemini,groq,cerebras)
# Only providers with valid API keys will be used
#LLM_PROVIDERS=gemini,groq,cerebras

# Model fallback chains (comma-separated: primary,fallback1,fallback2,...)
#CEREBRAS_EXPANDER_MODELS=llama-3.3-70b,llama-3.1-8b
#CEREBRAS_INTENT_MODELS=llama-3.3-70b,llama-3.1-8b
#GEMINI_EXPANDER_MODELS=gemini-2.5-flash,gemini-2.5-flash-lite
#GEMINI_INTENT_MODELS=gemini-2.5-flash,gemini-2.5-flash-lite
#GROQ_EXPANDER_MODELS=meta-llama/llama-4-scout-17b-16e-instruct,llama-3.1-8b-instant
#GROQ_INTENT_MODELS=meta-llama/llama-4-maverick-17b-128e-instruct,llama-3.3-70b-versatile

# ------------------------------------------------------------------------------
# Server Configuration
# ------------------------------------------------------------------------------
#LOG_LEVEL=info
#PORT=10000
#SHUTDOWN_TIMEOUT=30s

# ------------------------------------------------------------------------------
# Better Stack Logging (Optional)
# Leave token empty to disable Better Stack log shipping
# ------------------------------------------------------------------------------
#BETTERSTACK_ENDPOINT=https://in.logs.betterstack.com
#BETTERSTACK_SOURCE_TOKEN=your_betterstack_source_token_here

# ------------------------------------------------------------------------------
# Sentry Error Tracking (Optional)
# Leave DSN empty to disable error tracking
# Use your Sentry project's DSN
# DSN format: https://$APPLICATION_TOKEN@$INGESTING_HOST/1
# ------------------------------------------------------------------------------
#SENTRY_DSN=https://your_sentry_application_token_here@sentry.example.com/1
#SENTRY_ENVIRONMENT=production
#SENTRY_RELEASE=ntpu-linebot-go@1.0.0
#SENTRY_SAMPLE_RATE=1.0
#SENTRY_TRACES_SAMPLE_RATE=0.0

# ------------------------------------------------------------------------------
# Data Configuration
# ------------------------------------------------------------------------------
# Cache TTL for contacts/courses/syllabi (default: 168h = 7 days)
#CACHE_TTL=168h

# Database and cache storage directory
# Default: ./data (Windows), /data (Linux/Mac)
#DATA_DIR=/data

# ------------------------------------------------------------------------------
# Scraper Configuration
# ------------------------------------------------------------------------------
#SCRAPER_MAX_RETRIES=10
#SCRAPER_TIMEOUT=60s

# ------------------------------------------------------------------------------
# Webhook Configuration
# ------------------------------------------------------------------------------
#WEBHOOK_TIMEOUT=60s

# ------------------------------------------------------------------------------
# Rate Limit Configuration (Token Bucket Algorithm)
# ------------------------------------------------------------------------------
# Global rate limit across all users
#GLOBAL_RATE_RPS=100

# Per-user webhook request limiting
#   Burst: 15 tokens (can send 15 requests instantly)
#   Refill: 0.1 tokens/sec (1 token per 10 seconds)
#USER_RATE_BURST=15
#USER_RATE_REFILL=0.1

# Per-user LLM API limiting (Multi-Layer: Hourly + Daily)
#   Burst: 60 tokens (maximum instant AI requests)
#   Refill: 30 tokens/hour
#   Daily: 180 requests/day (sliding window)
#LLM_RATE_BURST=60
#LLM_RATE_DAILY=180
#LLM_RATE_REFILL=30

# ------------------------------------------------------------------------------
# Startup Configuration
# ------------------------------------------------------------------------------
# Grace period for warmup (only applies when WAIT_FOR_WARMUP=true)
# After this period, traffic is allowed even if warmup is still in progress
#WARMUP_GRACE_PERIOD=10m

# Wait for initial warmup before accepting traffic
#   true: Wait for warmup completion or grace period timeout
#   false (default): Accept traffic immediately after database connection
#WAIT_FOR_WARMUP=false

# ------------------------------------------------------------------------------
# Metrics Authentication (Optional)
# ------------------------------------------------------------------------------
# Basic Auth for /metrics endpoint (leave password empty to disable)
#METRICS_PASSWORD=your_secure_password_here
#METRICS_USERNAME=prometheus
