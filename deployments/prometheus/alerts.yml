# NTPU LINE Bot Prometheus Alerting Rules
#
# Design Philosophy (2025):
# - Alert on symptoms, not causes (user-facing impact)
# - Avoid alert fatigue: meaningful thresholds with appropriate durations
# - No recording rules: keep it simple, Prometheus handles query efficiency
# - Follow RED/USE methodology for service/resource alerts

groups:
  # ============================================
  # Service Alerts (RED Method - Rate, Errors, Duration)
  # ============================================
  - name: ntpu_service_alerts
    rules:
      # Critical: Service completely down
      - alert: ServiceDown
        expr: up{job="ntpu-linebot"} == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "NTPU LINE Bot service is down"
          description: "Service has been unreachable for more than 2 minutes."

      # Webhook latency (LINE best practice: < 2s response)
      - alert: WebhookLatencyHigh
        expr: histogram_quantile(0.95, sum(rate(ntpu_webhook_duration_seconds_bucket[5m])) by (le)) > 2
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Webhook P95 latency exceeds 2 seconds"
          description: "P95 latency is {{ $value | printf \"%.2f\" }}s, which may cause LINE timeout issues."

      # Webhook error rate
      - alert: WebhookErrorRateHigh
        expr: |
          (
            sum(rate(ntpu_webhook_total{status!="success"}[5m]))
            / sum(rate(ntpu_webhook_total[5m]))
          ) > 0.05
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Webhook error rate exceeds 5%"
          description: "Error rate is {{ $value | humanizePercentage }} over the last 5 minutes."

      # Scraper failure rate (external dependency)
      - alert: ScraperFailureRateHigh
        expr: |
          (
            sum by (module) (rate(ntpu_scraper_total{status!="success"}[5m]))
            / sum by (module) (rate(ntpu_scraper_total[5m]))
          ) > 0.3
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Scraper failure rate exceeds 30% for {{ $labels.module }}"
          description: "Module {{ $labels.module }} has {{ $value | humanizePercentage }} failure rate."

      # Scraper latency (slow external calls)
      - alert: ScraperLatencyHigh
        expr: histogram_quantile(0.95, sum by (le, module) (rate(ntpu_scraper_duration_seconds_bucket[5m]))) > 30
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Scraper P95 latency exceeds 30 seconds for {{ $labels.module }}"
          description: "Module {{ $labels.module }} P95 latency is {{ $value | printf \"%.1f\" }}s."

  # ============================================
  # Cache Alerts (USE Method - Utilization)
  # ============================================
  - name: ntpu_cache_alerts
    rules:
      # Low cache hit rate (data freshness issue)
      - alert: CacheHitRateLow
        expr: |
          (
            sum(rate(ntpu_cache_operations_total{result="hit"}[5m]))
            / sum(rate(ntpu_cache_operations_total[5m]))
          ) < 0.5
        for: 1h
        labels:
          severity: info
        annotations:
          summary: "Overall cache hit rate below 50%"
          description: "Cache hit rate is {{ $value | humanizePercentage }}. Consider running warmup."

  # ============================================
  # LLM Alerts (GenAI - Reliability)
  # ============================================
  - name: ntpu_llm_alerts
    rules:
      # LLM error rate (only count actual errors, not clarifications)
      - alert: LLMErrorRateHigh
        expr: |
          (
            sum(rate(ntpu_llm_total{status=~"error|fallback"}[5m]))
            / sum(rate(ntpu_llm_total[5m]))
          ) > 0.2
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "LLM error rate exceeds 20%"
          description: "LLM API error rate is {{ $value | humanizePercentage }}."

      # LLM latency
      - alert: LLMLatencyHigh
        expr: histogram_quantile(0.95, sum(rate(ntpu_llm_duration_seconds_bucket[5m])) by (le)) > 5
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "LLM P95 latency exceeds 5 seconds"
          description: "Gemini API P95 latency is {{ $value | printf \"%.1f\" }}s."

  # ============================================
  # Search Alerts (BM25 Index)
  # ============================================
  - name: ntpu_search_alerts
    rules:
      # Empty search index
      - alert: SearchIndexEmpty
        expr: sum(ntpu_index_size{index="bm25"}) == 0
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: "BM25 search index is empty"
          description: "No documents in search index. Smart search will not work."

      # Search latency
      - alert: SearchLatencyHigh
        expr: histogram_quantile(0.95, sum(rate(ntpu_search_duration_seconds_bucket[5m])) by (le)) > 3
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Search P95 latency exceeds 3 seconds"
          description: "Search P95 latency is {{ $value | printf \"%.1f\" }}s."

  # ============================================
  # Rate Limiter Alerts
  # ============================================
  - name: ntpu_ratelimit_alerts
    rules:
      # Rate limiter dropping requests
      - alert: RateLimiterDroppingRequests
        expr: sum(rate(ntpu_rate_limiter_dropped_total[5m])) > 0
        for: 5m
        labels:
          severity: info
        annotations:
          summary: "Rate limiter is dropping requests"
          description: "{{ $value | printf \"%.2f\" }} requests/sec being dropped."

  # ============================================
  # Background Job Alerts
  # ============================================
  - name: ntpu_job_alerts
    rules:
      # Warmup job taking too long (any module)
      - alert: WarmupJobSlow
        expr: histogram_quantile(0.95, sum by (le, module) (rate(ntpu_job_duration_seconds_bucket{job="warmup"}[1h]))) > 1800
        for: 15m
        labels:
          severity: info
        annotations:
          summary: "Warmup job P95 duration exceeds 30 minutes for {{ $labels.module }}"
          description: "Module {{ $labels.module }} warmup taking {{ $value | printf \"%.0f\" }}s."

      # Cleanup job taking too long
      - alert: CleanupJobSlow
        expr: histogram_quantile(0.95, sum(rate(ntpu_job_duration_seconds_bucket{job="cleanup",module="all"}[1h])) by (le)) > 300
        for: 15m
        labels:
          severity: info
        annotations:
          summary: "Cleanup job P95 duration exceeds 5 minutes"
          description: "Cleanup taking {{ $value | printf \"%.0f\" }}s."

      # Sticker refresh job taking too long
      - alert: StickerRefreshJobSlow
        expr: histogram_quantile(0.95, sum(rate(ntpu_job_duration_seconds_bucket{job="sticker_refresh",module="all"}[1h])) by (le)) > 300
        for: 15m
        labels:
          severity: info
        annotations:
          summary: "Sticker refresh job P95 duration exceeds 5 minutes"
          description: "Sticker refresh taking {{ $value | printf \"%.0f\" }}s."

  # ============================================
  # System Alerts (Go Runtime)
  # ============================================
  - name: ntpu_system_alerts
    rules:
      # High memory usage (approaching container limit)
      - alert: HighMemoryUsage
        expr: go_memstats_alloc_bytes{job="ntpu-linebot"} > 400 * 1024 * 1024
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Memory usage exceeds 400MB"
          description: "Current memory: {{ $value | humanize1024 }}B (limit: 512MB)."

      # High goroutine count (potential leak)
      - alert: HighGoroutineCount
        expr: go_goroutines{job="ntpu-linebot"} > 1000
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Goroutine count exceeds 1000"
          description: "Current count: {{ $value }}. Possible goroutine leak."
