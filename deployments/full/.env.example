# ==============================================================================
# Full Stack Deployment Configuration
# Bot + Prometheus + Grafana + Alertmanager (internal Docker network)
# ==============================================================================

# ------------------------------------------------------------------------------
# LINE Bot Configuration (Required)
# ------------------------------------------------------------------------------
LINE_CHANNEL_ACCESS_TOKEN=your_access_token_here
LINE_CHANNEL_SECRET=your_channel_secret_here

# ------------------------------------------------------------------------------
# LLM Configuration (Optional)
# At least one API key required to enable NLU intent parsing and query expansion
# ------------------------------------------------------------------------------
# Get Gemini API key from: https://aistudio.google.com/apikey
#GEMINI_API_KEY=your_gemini_api_key_here

# Get Groq API key from: https://console.groq.com/keys
#GROQ_API_KEY=your_groq_api_key_here

# Get Cerebras API key from: https://cloud.cerebras.ai/
#CEREBRAS_API_KEY=your_cerebras_api_key_here

# Provider fallback order (comma-separated, default: gemini,groq,cerebras)
# Only providers with valid API keys will be used
#LLM_PROVIDERS=gemini,groq,cerebras

# Model fallback chains (comma-separated: primary,fallback1,fallback2,...)
#GEMINI_INTENT_MODELS=gemini-2.5-flash,gemini-2.5-flash-lite
#GEMINI_EXPANDER_MODELS=gemini-2.5-flash,gemini-2.5-flash-lite
#GROQ_INTENT_MODELS=meta-llama/llama-4-maverick-17b-128e-instruct,llama-3.3-70b-versatile
#GROQ_EXPANDER_MODELS=meta-llama/llama-4-scout-17b-16e-instruct,llama-3.1-8b-instant
#CEREBRAS_INTENT_MODELS=llama-3.3-70b,llama-3.1-8b
#CEREBRAS_EXPANDER_MODELS=llama-3.3-70b,llama-3.1-8b

# ------------------------------------------------------------------------------
# Server Configuration
# ------------------------------------------------------------------------------
#PORT=10000
#LOG_LEVEL=info
#SHUTDOWN_TIMEOUT=30s

# ------------------------------------------------------------------------------
# Better Stack Logging (Optional)
# Leave token empty to disable Better Stack log shipping
# ------------------------------------------------------------------------------
#BETTERSTACK_SOURCE_TOKEN=your_betterstack_source_token_here
#BETTERSTACK_ENDPOINT=https://in.logs.betterstack.com

# ------------------------------------------------------------------------------
# Data Configuration
# ------------------------------------------------------------------------------
# Database and cache storage directory
# Default: ./data (Windows), /data (Linux/Mac)
#DATA_DIR=/data

# Cache TTL for contacts/courses/syllabi (default: 168h = 7 days)
#CACHE_TTL=168h

# ------------------------------------------------------------------------------
# Scraper Configuration
# ------------------------------------------------------------------------------
#SCRAPER_TIMEOUT=60s
#SCRAPER_MAX_RETRIES=10

# ------------------------------------------------------------------------------
# Webhook Configuration
# ------------------------------------------------------------------------------
#WEBHOOK_TIMEOUT=60s

# ------------------------------------------------------------------------------
# Rate Limit Configuration (Token Bucket Algorithm)
# ------------------------------------------------------------------------------
# Per-user webhook request limiting
#   Burst: 15 tokens (can send 15 requests instantly)
#   Refill: 0.1 tokens/sec (1 token per 10 seconds)
#USER_RATE_BURST=15
#USER_RATE_REFILL=0.1

# Per-user LLM API limiting (Multi-Layer: Hourly + Daily)
#   Burst: 60 tokens (maximum instant AI requests)
#   Refill: 30 tokens/hour
#   Daily: 180 requests/day (sliding window)
#LLM_RATE_BURST=60
#LLM_RATE_REFILL=30
#LLM_RATE_DAILY=180

# Global rate limit across all users
#GLOBAL_RATE_RPS=100

# ------------------------------------------------------------------------------
# Startup Configuration
# ------------------------------------------------------------------------------
# Wait for initial warmup before accepting traffic
#   true: Wait for warmup completion or grace period timeout
#   false (default): Accept traffic immediately after database connection
#WAIT_FOR_WARMUP=false

# Grace period for warmup (only applies when WAIT_FOR_WARMUP=true)
# After this period, traffic is allowed even if warmup is still in progress
#WARMUP_GRACE_PERIOD=10m

# ------------------------------------------------------------------------------
# Metrics Authentication (Internal network - disabled by default)
# ------------------------------------------------------------------------------
# Basic Auth for /metrics endpoint (leave password empty to disable)
METRICS_USERNAME=prometheus
METRICS_PASSWORD=

# ------------------------------------------------------------------------------
# Docker Configuration
# ------------------------------------------------------------------------------
#IMAGE_TAG=latest
#HOST_PORT=10000

# ------------------------------------------------------------------------------
# Monitoring Stack Configuration
# ------------------------------------------------------------------------------
GRAFANA_USER=admin
GRAFANA_PASSWORD=admin123

#GRAFANA_PORT=3000
#PROMETHEUS_PORT=9090
#ALERTMANAGER_PORT=9093
